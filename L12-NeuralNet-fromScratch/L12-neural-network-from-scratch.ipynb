{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 12 - Building a Neural Network Framework from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    Declare a class, node, with the following properties:\n",
    "    -- inputs\n",
    "    -- outputs\n",
    "    -- calculate and forward values to the next node\n",
    "    -- receive values from backward-propagation (given by partial derivatives)\n",
    "        to adjust parameters (\"gradients\")\n",
    "    \"\"\"\n",
    "    def __init__(self, inputs=None):\n",
    "        inputs = inputs or []\n",
    "        self.inputs = inputs\n",
    "        self.outputs = []\n",
    "        \n",
    "        for n in self.inputs:\n",
    "            n.outputs.append(self)\n",
    "            # 'self' node as inbound nodes' outbound nodes\n",
    "             \n",
    "            self.value = None\n",
    "            \n",
    "            self.gradients = {\n",
    "                # a dictionary where the key is \"self\",\n",
    "                # and the value is the partial derivative of \"self\"\n",
    "                # if the functional form is wx + b, then the partial derivatives are:\n",
    "                # w: x\n",
    "                # x: w\n",
    "                # b: 1\n",
    "                \n",
    "            }\n",
    "    \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Forward propagation\n",
    "        Computing the output value based on inbound nodes and store the results \n",
    "        in self.value\n",
    "        \"\"\"\n",
    "        raise NotImplemented\n",
    "        \n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Backward propagation\n",
    "        \"\"\"\n",
    "        raise NotImplemented\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Input(Node):\n",
    "    \"\"\"\n",
    "    Defining the input nodes\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        An input node has no inbound nodes,\n",
    "        so no need to pass anything to the 'Node' instantiator\n",
    "        \"\"\"\n",
    "        Node.__init__(self)\n",
    "        \n",
    "    def forward(self, value=None):\n",
    "        \"\"\"\n",
    "        Only input node is the node where the value can be passed as\n",
    "        an argument to forward(), since it has no inbound nodes;\n",
    "        all other node implementations should receive the values from\n",
    "        the previous node\n",
    "        \"\"\"\n",
    "        if value is not None:\n",
    "            self.value = value\n",
    "        \n",
    "    def backward(self):\n",
    "        self.gradients = {self:0}\n",
    "        for n in self.outputs:\n",
    "            grad_cost = n.gradients[self]\n",
    "            self.gradients[self] = grad_cost * 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Add(Node):\n",
    "    \"\"\"\n",
    "    Define a subclass of \"Node\" where the calculation is simply adding all inputs.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, nodes):\n",
    "        Node.__init__(self, nodes)\n",
    "        \n",
    "    def forward(self):\n",
    "        self.value = sum([n.value for n in self.inputs])\n",
    "        # alternatively, self.value = sum(map(lamda n:n.value, self.inputs))\n",
    "    \n",
    "    # note there isn't a backpropagation process since the function, \"add\",\n",
    "    # does not have any parameters (and thus cannot calculate partial derivatives)!       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Node):\n",
    "    \"\"\"\n",
    "    Define a subclss of \"Node\" where the calculation is a linear function \n",
    "    f(X) = wX + b. (X is a vector)\n",
    "    \"\"\"\n",
    "    def __init__(self, nodes, weights, bias):\n",
    "        Node.__init__(self, [nodes, weights, bias])\n",
    "        # initialize the Linear Node with a list where \n",
    "        #  -- nodes = inbound nodes vector\n",
    "        #  -- weights = parameter vector W\n",
    "        #  -- bias = intercept vector b\n",
    "            \n",
    "    def forward(self):\n",
    "        inputs = self.inputs[0].value\n",
    "        weights = self.inputs[1].value\n",
    "        bias = self.inputs[2].value\n",
    "        \n",
    "        self.value = np.dot(inputs, weights) + bias\n",
    "        \n",
    "    def backward(self):\n",
    "        \n",
    "        # initialize a partial for each of the inbound nodes\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inputs}\n",
    "        \n",
    "        for n in self.outputs:\n",
    "            # get the partial (grad_cost) with regard to this node\n",
    "            grad_cost = n.gradients[self] \n",
    "            \n",
    "            self.gradients[self.inputs[0]] = np.dot(\n",
    "                grad_cost, self.inputs[1].value.T\n",
    "            ) # the partials of f(X) on X are weights W (self.input[1])\n",
    "            \n",
    "            self.gradients[self.inputs[1]] = np.dot(\n",
    "                grad_cost, self.inputs[0].value.T\n",
    "            ) # the partials of f(X) on W are X (self.input[0])\n",
    "            \n",
    "            self.gradients[self.inputs[2]] = np.dot(\n",
    "                grad_cost, 1 # the partial derivative of f(X) on b = 1\n",
    "            )\n",
    "            \n",
    "            # alternatively, \n",
    "            # self.gradients[self.inputs[2]] = np.sum(grad_cost, axis=0, keepdims=False)\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sigmoid(Node):\n",
    "    \"\"\"\n",
    "    Define a subclss of \"Node\" where the calculation is a sigmoid function \n",
    "    f(x) = 1 / 1 + e^(-x)\n",
    "    \"\"\"\n",
    "    def __init__(self, node):\n",
    "        Node.__init__(self, [node])\n",
    "        \n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-1 * x))\n",
    "    \n",
    "    def forward(self):\n",
    "        self.x = self.inputs[0].value\n",
    "        self.value = self._sigmoid(self.x)\n",
    "        \n",
    "    def backward(self):\n",
    "        self.partial = self._sigmoid(self.x) * (1 - self._sigmoid(self.x))\n",
    "        \n",
    "        # note that for a sigmoid function, its derivative has the following property:\n",
    "        # f'(x) = f(x)[1-f(x)]\n",
    "        \n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inputs}\n",
    "        \n",
    "        for n in self.outputs:\n",
    "            # get the partial with respect to this node\n",
    "            grad_cost = n.gradients[self]\n",
    "            \n",
    "            self.gradients[self.inputs[0]] = grad_cost * self.partial\n",
    "            # note - use * to keep all the dimensions consistent!\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE(Node):\n",
    "    \"\"\"\n",
    "    Define the loss function\n",
    "    \"\"\"\n",
    "    def __init__(self, y_true, y_hat):\n",
    "        Node.__init__(self, [y_true, y_hat])\n",
    "        \n",
    "    def forward(self):\n",
    "        y_true = self.inputs[0].value.reshape(-1, 1) \n",
    "        y_hat = self.inputs[0].value.reshape(-1, 1)\n",
    "        # reshape the value to fall in the range of [-1,1]\n",
    "        assert(y_true.shape == y_hat.shape)\n",
    "        \n",
    "        self.n = self.inputs[0].value.shape[0] # # of observations (length of vector y)\n",
    "        self.diff = y_true - y_hat\n",
    "        \n",
    "        self.value = np.mean(self.diff ** 2) # mean std. err\n",
    "        \n",
    "    def backward(self):    \n",
    "        self.gradients[self.inputs[0]] = (2 / n) * self.diff # the partial of MSE on y_true\n",
    "        self.gradients[self.inputs[1]] = -1 * (2 / n) * self.diff # the partial of MSE on y_hat\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_epoch(output_node, graph: list):\n",
    "    \"\"\"\n",
    "    Define a procedure to connect all the nodes defined above\n",
    "    (i.e., run the neural net for one round --- forward and backward)\n",
    "    \"\"\"\n",
    "    for n in graph:\n",
    "        n.forward()\n",
    "        # each node execute forward,\n",
    "        # get self.value based on the topological sorted graph\n",
    "        \n",
    "    for n in graph[::-1]:\n",
    "        n.backward()\n",
    "\n",
    "# Note:\n",
    "# in practice, it is common to feed in multiple data examples in each epoch,\n",
    "# since they can be processes in parallel;\n",
    "# the number of examples is called 'batch size'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topological_sort(graph):\n",
    "    \"\"\"\n",
    "    Define a topological sort procedure where the input is a @graph,\n",
    "    and the output is a @sorted_list\n",
    "    \"\"\"\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_update(trainable_nodes, learning_rate=1e-3):\n",
    "    for node in trainable_nodes:\n",
    "        update_value += -1 *(learning_rate * node.gradient[node])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis=0) # normalize X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = Input(), Input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1, b1 = Input(), Input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2, b2 = Input(), Input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_1 = Linear(X_, W1, b1)\n",
    "sigmoid_1 = Sigmoid(loss1)\n",
    "linear2 = Linear(sigmoid_1, W2, b2)\n",
    "loss = MES(y, linear2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample, shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topological_sorted_list = topological_sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "batch_size = 16  # we supply 16 values for each input\n",
    "batch_num = X_.shape[0] / batch_size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    \n",
    "    for batch in range(batch_num):\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
