{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 12 - Building a Neural Network Framework from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    Declare a class, node, with the following properties:\n",
    "    -- inputs\n",
    "    -- outputs\n",
    "    -- calculate and forward values to the next node\n",
    "    -- receive values from backward-propagation (given by partial derivatives)\n",
    "        to adjust parameters (\"gradients\")\n",
    "    \"\"\"\n",
    "    def __init__(self, inputs=None):\n",
    "        inputs = inputs or []\n",
    "        self.inputs = inputs\n",
    "        self.outputs = []\n",
    "        \n",
    "        for n in self.inputs:\n",
    "            n.outputs.append(self)\n",
    "            \n",
    "            self.value = None\n",
    "            \n",
    "            self.gradients = {\n",
    "                # a dictionary where the key is \"self\",\n",
    "                # and the value is the partial derivative of \"self\"\n",
    "                # if the functional form is wx + b, then the partial derivatives are:\n",
    "                # w: x\n",
    "                # x: w\n",
    "                # b: 1\n",
    "                \n",
    "            }\n",
    "    \n",
    "    def calculate(self):\n",
    "        \n",
    "        raise NotImplemented\n",
    "        \n",
    "    def backward_partial(self):\n",
    "        \n",
    "        return NotImplemented\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Input(Node):\n",
    "    def __init__(self):\n",
    "        Node.__init__(self)\n",
    "        \n",
    "    def calculate(self, value=None):\n",
    "        self.value = value\n",
    "        \n",
    "    def backward_partial(self):\n",
    "        \n",
    "        for n in self.outputs:\n",
    "            self.gradients[self] = n.gradients[self] * 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Add(Node):\n",
    "    \"\"\"\n",
    "    Define a subclss of \"Node\" where the calculation is simply adding all inputs.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, nodes):\n",
    "        Node.__init__(self, nodes)\n",
    "        \n",
    "    def calculate(self):\n",
    "        self.value = sum([n.value for n in self.inputs])\n",
    "    \n",
    "    # note there isn't a backpropagation process since the function, \"add\",\n",
    "    # does not have any parameters (and thus cannot calculate partial derivatives)!       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Node):\n",
    "    \"\"\"\n",
    "    Define a subclss of \"Node\" where the calculation is a linear function \n",
    "    f(X) = wX + b. (X is a vector)\n",
    "    \"\"\"\n",
    "    def __init__(self, nodes, weights, bias):\n",
    "        Node.__init__(nodes)\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "            \n",
    "    def calculate(self):\n",
    "        x = self.inputs\n",
    "        self.value = np.dot(weights, x) + bias\n",
    "        \n",
    "    def backward_partial(self):\n",
    "        \n",
    "        for n in self.outputs:\n",
    "            \n",
    "            grad_cost = n.gradients[self] \n",
    "            \n",
    "            self.gradients[self.inputs] = np.dot(\n",
    "                grad_cost, self.weights.value.T\n",
    "            )\n",
    "            \n",
    "            self.gradients[self.weights] = np.dot(\n",
    "                grad_cost, self.inputs.value.T\n",
    "            )\n",
    "            \n",
    "            self.gradients[self.bias] = np.dot(\n",
    "                grad_cost, 1 # the partial derivative of f(X) on b = 1\n",
    "            )\n",
    "                \n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sigmoid(Node):\n",
    "    \"\"\"\n",
    "    Define a subclss of \"Node\" where the calculation is a sigmoid function \n",
    "    f(x) = 1 / 1 + e^(-x)\n",
    "    \"\"\"\n",
    "    def __init__(self, node):\n",
    "        Node.__init__(Node)\n",
    "        \n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-1 * x))\n",
    "    \n",
    "    def calculate(self):\n",
    "        self.x = self.inputs[0].value\n",
    "        self.value = self._sigmoid(self.x)\n",
    "        \n",
    "    # note that for a sigmoid function, its derivative has the following property:\n",
    "    # f'(x) = f(x)[1-f(x)]\n",
    "        \n",
    "    def backward_partial(self):\n",
    "        self.partial = self._sigmoid(self.x) * (1 - self._sigmoid(self.x))\n",
    "        \n",
    "        for n in self.outputs:\n",
    "            grad_cost = n.gradients[self]\n",
    "            \n",
    "            self.gradients[self.inputs[0]] = grad_cost * self.partial\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LOSS(Node):\n",
    "    \"\"\"\n",
    "    Define the loss function\n",
    "    \"\"\"\n",
    "    def __init__(self, y_true, y_hat):\n",
    "        Node.__init__([y_true, y_hat])\n",
    "        \n",
    "    def calculate(self):\n",
    "        y_true = self.inputs[0].value.reshape(-1, 1) \n",
    "        # reshape the value to fall in the range of [-1,1]\n",
    "        y_hat = self.inputs[0].value.reshape(-1, 1)\n",
    "        \n",
    "        self.diff = y_true - y_hat\n",
    "        \n",
    "        self.value = np.mean(self.diff ** 2)\n",
    "        \n",
    "    def backward_partial(self):\n",
    "        n = self.inputs[0].value.shape[0]\n",
    "        \n",
    "        self.gradients[self.inputs[0]] = (2 / n) * self.diff\n",
    "        self.gradients[self.inputs[1]] = -1 * (2 / n) * self.diff\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_epoch(output_node, topological_sorted_graph: list):\n",
    "    \"\"\"\n",
    "    Define a procedure to connect all the nodes defined above\n",
    "    (i.e., run the neural net for one round --- forward and backward)\n",
    "    \"\"\"\n",
    "    for n in topological_sorted_graph:\n",
    "        n.forward()\n",
    "        \n",
    "    for n in topological_sorted_graph[::-1]:\n",
    "        n.backward()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topological_sort(graph):\n",
    "    \"\"\"\n",
    "    Define a topological sort procedure where the input is a @graph,\n",
    "    and the output is a @sorted_list\n",
    "    \"\"\"\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_update(trainable_nodes, learning_rate=1e-3):\n",
    "    for node in trainable_nodes:\n",
    "        update_value += -1 *(learning_rate * node.gradient[node])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis=0) # normalize X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = Input(), Input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1, b1 = Input(), Input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2, b2 = Input(), Input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_1 = Linear(X_, W1, b1)\n",
    "sigmoid_1 = Sigmoid(loss1)\n",
    "linear2 = Linear(sigmoid_1, W2, b2)\n",
    "loss = MES(y, linear2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample, shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topological_sorted_list = topological_sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "batch_size = 16  # we supply 16 values for each input\n",
    "batch_num = X_.shape[0] / batch_size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    \n",
    "    for batch in range(batch_num):\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
